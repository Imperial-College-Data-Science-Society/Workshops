{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Donald Trump tweet generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating a model that learns from text input and generates more text on the fly.\n",
    "\n",
    "Using a Donald Trump tweet dataset we'll see if our model can successfully generate Trump-like tweets on the fly.\n",
    "\n",
    "The model will be using character based predictions to predict the next most likely character given a sequence of characters. For example, given the sequence `Donald Trum`, it should ideally predict `p`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of tweets 5674 tweets from Donald Trump.\n",
    "Originally taken from https://www.kaggle.com/ayushggarg/all-trumps-twitter-insults-20152021\n",
    "\n",
    "We create a `TweetDataset` class that inhertits from PyTorch's `Dataset` class.\n",
    "In doing so, we must overload the following methods:\n",
    "- `__len__(..)`: return the number of training samples\n",
    "- `__getitem__(..)`: return sample with given index\n",
    "\n",
    "We don't train on a per-tweet basis but on a per-sequence basis. That is we define a sequence to be a string of `seq_len` characters which we will feed in as training data.\n",
    "In the constructor, we therefore split all our tweets into sequences which we space-pad if too short.\n",
    "\n",
    "Note that if our sequence length is 4, we must actually sample from a string of 5 characters. For instance, the string `hello` will give input sequence `hell` and target sequence `ello`, shifted by one since we feed in a character to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, path, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.data = pd.read_csv(path)\n",
    "        \n",
    "        self.chars = self._get_unique_chars()\n",
    "        self.vocab_size = len(self.chars)\n",
    "        print('Vocab size: ' + str(self.vocab_size))\n",
    "        print(self.chars)\n",
    "        \n",
    "        # Dicts used to assign a unique number to every distinct char in data\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = { c: i for i, c in self.int2char.items() }\n",
    "        \n",
    "        # List of pairs [[input, target], ...]\n",
    "        self.sequences = self._create_sequences(self.seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "        \n",
    "    # Returns input and target sequences for given index\n",
    "    # input is of shape [seq_len, vocab_size] and is one-hot encoded\n",
    "    # target is of shape [seq_len] and is NOT one-hot encoded\n",
    "    def __getitem__(self, idx):\n",
    "        inpt = self.sequences[idx][0] # Get input sequence\n",
    "        inpt = self._encode_sequence(inpt) # Encode every char into its integer representation\n",
    "        inpt = self._one_hot_sequence(inpt, self.vocab_size) # One hot every integer\n",
    "        \n",
    "        target = self.sequences[idx][1]\n",
    "        target = self._encode_sequence(target)\n",
    "        \n",
    "        inpt = torch.Tensor(inpt)\n",
    "        target = torch.Tensor(target)\n",
    "        \n",
    "        return inpt, target\n",
    "    \n",
    "    # Encodes a given sequence.\n",
    "    def _encode_sequence(self, seq):\n",
    "        return [self.char2int[c] for c in seq]\n",
    "    \n",
    "    # One hot encodes a single integer. Returns a numpy vector of size VOCAB_SIZE\n",
    "    def _one_hot_sequence(self, seq, vocab_size):\n",
    "        enc = []\n",
    "\n",
    "        for s in seq:\n",
    "            vec = [0] * vocab_size\n",
    "            vec[s] = 1\n",
    "            enc.append(vec)\n",
    "\n",
    "        return enc\n",
    "    \n",
    "    def _create_sequences(self, seq_len):\n",
    "        seqs = []\n",
    "        for tweet in self.data['tweet'].values:\n",
    "            for i in range(0, len(tweet), seq_len + 1):\n",
    "                seq = tweet[i:i+seq_len+1]\n",
    "\n",
    "                # Pad sequence if needed\n",
    "                if len(seq) < seq_len+1:\n",
    "                    seq += ' ' * (seq_len+1 - len(seq))\n",
    "\n",
    "                assert(len(seq) == seq_len+1)\n",
    "\n",
    "                seqs.append(seq)\n",
    "                \n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        for seq in seqs:\n",
    "            inputs.append(seq[:-1])\n",
    "            targets.append(seq[1:])\n",
    "            \n",
    "        return list(zip(inputs, targets))\n",
    "    \n",
    "    def _get_unique_chars(self):\n",
    "        chars = set()\n",
    "\n",
    "        for tweet in self.data['tweet'].values:\n",
    "            for char in tweet:\n",
    "                chars.add(char)\n",
    "\n",
    "        chars = list(chars)\n",
    "        chars.sort()\n",
    "\n",
    "        return chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple RNN with one GRU cell that then feeds into a fully connected layer for output.\n",
    "\n",
    "The `input_size` is the size of the input to the GRU cell. In our case this will be the size of our vocab list, or in other words the number of possible characters we're using.\n",
    "\n",
    "`hidden_size` is the size of the hidden vector of the GRU cell which recurs at each step. We can try out different sizes to see if performance changes.\n",
    "\n",
    "`output_size` is the output size of the fully connected layer that takes in as input the GRU hidden state and generates the output of the model. Since we're predicting characters, this will also have to be the size of our vocab list.\n",
    "\n",
    "`n_layers` is the number of GRU cells we want. We'll stick with 1 for now as stacking multiple cells slows down training quite substantially. Fell free to experiment aside.\n",
    "\n",
    "The `init_hidden(..)` method is used to initialise the hidden layer of the GRU cell. We start off with a zero matrix.\n",
    "\n",
    "Note that the `forward` method takes as input arguments `x` (the input) and `hidden` (the hidden layer) and returns an output along with the *updated* hidden layer. This is what allows us to recur by passing the hidden layer around at every forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        # Size of input vector (= number of possible characters)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Size of output vector (= number of possible characters)\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # With batch_first off, expected input shape is [seq_len, batch_size,\n",
    "        # input_size]. With on, it's [batch_size, seq_len, input_size]\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Shape of x: [batch_size, seq_len, input_size]\n",
    "    # Shape of hidden: [n_layers, batch_size, hidden_size]\n",
    "    # Shape of output: [batch_size, input_size]\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a few hyperparameters which we'll be able to tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10\n",
    "PRINT_EVERY = 1\n",
    "LEARNING_RATE = 0.001\n",
    "SEQ_LEN = 32\n",
    "N_LAYERS = 1\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define our training loop, first iterating for the given number of epochs and for each epoch throught the entire dataset.\n",
    "\n",
    "The dataset gives us batches which we can feed into our model and then backpropagate.\n",
    "Note that the PyTorch model doesn't take in individual inputs and outputs but batches them directly for us, so `x` and `y` contain `BATCH_SIZE` samples which we all feed in at once.\n",
    "\n",
    "You can see that we initialise our hidden layer at the beginning and reset it for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, data, device='cpu'):\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        loss_avg = 0\n",
    "\n",
    "        # Iterate over batches\n",
    "        for i, batch in enumerate(data):\n",
    "            # Get input and target and send them to given device (cpu/gpu)\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Init a new hidden layer\n",
    "            hidden = model.init_hidden(BATCH_SIZE).to(device)\n",
    "            hidden.to(device)\n",
    "\n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Pass in batch\n",
    "            output, hidden = model(x, hidden)\n",
    "            output.to(device)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(output, y.view(-1).long())\n",
    "            loss_avg += loss\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_avg = loss_avg.item() / BATCH_SIZE\n",
    "        if epoch % PRINT_EVERY == 0:\n",
    "          print('Epoch ' + str(epoch) + ': ' + str(loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate all our classes and call our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TweetDataset('./trump.csv', SEQ_LEN)\n",
    "# Make dataloader out of our dataset, this will allow us to serve batches\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "# Train on GPU if possible\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Training on ' + str(device))\n",
    "\n",
    "# Create model\n",
    "input_size = dataset.vocab_size\n",
    "model = GRU(input_size, HIDDEN_SIZE, input_size, N_LAYERS)\n",
    "model.to(device)\n",
    "\n",
    "# Instantiate optimizer & loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "train(model, optimizer, loss_fn, dataloader)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(input_size, HIDDEN_SIZE, input_size, N_LAYERS)\n",
    "model.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has been trained we can use it to generate tweets on the fly!\n",
    "\n",
    "Remember that our model does character based prediction, we therefore need a start sequence to kick it off.\n",
    "We also ask for the length of the sequence we wish to generate.\n",
    "\n",
    "First we start by passing in all the characters of the start sequence, discarding the output but keeping the hidden state to initialise our GRU cell.\n",
    "\n",
    "Then for the length of the sequence, continuously predict the next character and feed it back in to get the next prediction and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, dataset, start, length):\n",
    "    # Format input\n",
    "    start = dataset._encode_sequence(start)\n",
    "    start = dataset._one_hot_sequence(start, dataset.vocab_size)\n",
    "    start = torch.Tensor(start)\n",
    "\n",
    "    # Init hidden layer\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden.to(device)\n",
    "\n",
    "    # Pass in start input, ignoring output\n",
    "    for s in start:\n",
    "        s = s.view(1, 1, -1)\n",
    "        _, hidden = model(s, hidden)\n",
    "\n",
    "    inpt = start[-1]\n",
    "    inpt = inpt.view(1, 1, -1)\n",
    "\n",
    "    res = ''\n",
    "    for i in range(length):\n",
    "        output, hidden = model(inpt, hidden)\n",
    "\n",
    "        # Discard batch size dim\n",
    "        output = output.view(-1)\n",
    "\n",
    "        # Get softmax distribution\n",
    "        output_dist = F.softmax(output, dim=0)\n",
    "\n",
    "        # Sample predicted char\n",
    "        top = torch.multinomial(output_dist, 1)[0]\n",
    "        top = top.detach().numpy()\n",
    "        top = top.item(0)\n",
    "        char = dataset.int2char[top]\n",
    "        res += char\n",
    "\n",
    "        # Create next input as one hot encoded vector of top char\n",
    "        inpt = dataset._encode_sequence(char)\n",
    "        inpt = dataset._one_hot_sequence(inpt, dataset.vocab_size)[0]\n",
    "        inpt = torch.Tensor(inpt)\n",
    "        inpt = inpt.view(1, 1, -1)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate(model, dataset, 'hello', 140)\n",
    "\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
